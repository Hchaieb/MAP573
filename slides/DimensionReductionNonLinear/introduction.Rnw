\begin{frame}[fragile]
  \partpage

\paragraph{Packages required for reproducing the slides}
<<load_packages, cache = FALSE, message=FALSE, warning=FALSE>>=
library(tidyverse)  # opinionated collection of packages for data manipulation
library(GGally)     # extension to ggplot vizualization system
library(FactoMineR) # PCA and oter linear method for dimension reduction
library(factoextra) # fancy plotting for FactoMineR output
# color and plots themes
library(RColorBrewer)
pal <- brewer.pal(10, "Set3")
theme_set(theme_bw())
@

\end{frame}

%% TODO find a fancy small data sets for illustration

\begin{frame}[fragile]
  \frametitle{Companion data set: '???'}
  \framesubtitle{the data }

\begin{block}{Description: \textcolor{black}{\it small data, medium-size}}
\small .\\
\end{block}

% \begin{figure}
%   \includegraphics[width=3cm]{crab}
%   \caption{A leptograpsus Crab}
% \end{figure}
\end{frame}

% \begin{frame}[fragile,allowframebreaks]
%   \frametitle{Companion data set: '??'}
%   \framesubtitle{Table header}
% 
% <<crabs dataset1>>=
% crabs <- MASS::crabs %>% select(-index) %>%
%   rename(sex = sex,
%          species         = sp,
%          frontal_lob     = FL,
%          rear_width      = RW,
%          carapace_length = CL,
%          carapace_width  = CW,
%          body_depth      = BD)
% crabs %>% select(sex, species) %>% summary() %>% knitr::kable("latex")
% dim(crabs)
% @
% 
% <<crabs dataset head>>=
% crabs %>% head(15) %>% knitr::kable("latex")
% @
% \end{frame}

\begin{frame}
  \frametitle{Dimension reduction: problem setup}

    \begin{block}{Settings}
      \begin{itemize}
        \item \alert{Training data} : $\mathcal{D}=\{\bx_1,\ldots,\bx_n\} \in \Rset^p$,   (i.i.d.)
        \item Space $\Rset^p$ of possibly high dimension $(n \ll p)$
      \end{itemize}
    \end{block}

    \vfill
    
    \begin{block}{Dimension Reduction Map}
       Construct a map $\Phi$ from the space $\Rset^{p}$ into a space $\Rset^{q}$ of \alert{smaller dimension}:
      \begin{align*}
          \Phi:\quad & \Rset^p \to \Rset^{q}, q \ll p\\
                     & \bx \mapsto \Phi(\bx)
      \end{align*}
    \end{block}
    
\end{frame}
 
\begin{frame}
  \frametitle{How should we design/construct $\Phi$?}

  \paragraph{Criterion}
  \begin{itemize}
    \item Geometrical approach
    \item \alert{\bf Reconstruction error}
    \item \alert{\bf Relationship preservation}
  \end{itemize}

  \vfill
  
  \paragraph{Form of the map $\Phi$}
  \begin{itemize}
    \item \alert{\bf Linear} or \alert{\bf non-linear ?}
    \item tradeoff between  interpretability and \alert{\bf versatility ?}
    \item tradeoff between  \alert{\bf high} or low computational resource
  \end{itemize}

\end{frame}

\subsection{Reconstruction error point of view}

\begin{frame}
  \frametitle{Reconstruction error approach}

  \begin{enumerate}
    \item  Construct a map $\Phi$ from the space $\Rset^{d}$ into a space $\Rset^{d'}$ of \alert{smaller dimension}:
      \begin{align*}
      \Phi:\quad & \Rset^d \to \Rset^{d'}, d' \ll d\\
               & \bx \mapsto \Phi(\bx)
      \end{align*}
    \item Construct $\widetilde{\Phi}$ from $\Rset^{d'}$ to $\Rset^d$ (\alert{reconstruction formula})
     \item Control an error between $\bx$ and its reconstruction $\tilde{\Phi}(\Phi(\bx))$, e.g
      \begin{equation*}
        \sum_{i=1}^n \left\| \bx_i - \tilde{\Phi}(\Phi(\bx_i)) \right\|^2
      \end{equation*}
  \end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Reconstruction error and PCA}

  \begin{block}{PCA Model}
    Linear model assumption
    \begin{equation*}
        \bx \simeq \bmu + \mathbf{F}_{1:d'} \bU_{1:d'}^\top
      \end{equation*}
      with $\bU$ orthonormal and no  constraint on $\mathbf{F}$
    \end{block}

  \begin{block}{Reconstruction error}
    In the case of PCA, then
    \begin{gather*}
      \Phi(\bx) = (\bx-\bmu)^\top \bU  \quad \text{and} \quad \tilde{\Phi}(\mathbf{F})= \bmu + \mathbf{F} \bU^\top  \\
      \frac{1}{n} \sum_{i=1}^n \| \bx_i  - ( \bmu + ( \bx_i -\bmu) \bU \bU^\top \|^2
    \end{gather*}
  \alert{Explicit solution:} $\bmu = \bar{x}$ the empirical mean
 and $\bU$ is an orthonormal basis of the space spanned by the $d'$
 first eigenvectors of the empirical covariance matrix
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Non linear extensions}

  Two directions
  \begin{enumerate}
    \item Non linear transformation of $\bx$ before PCA: kernel-PCA
    \item Other constrains on weigths $\bU$ or loadings $\mathbf{F}$: ICA, NMF, \dots
  \end{enumerate}

  \begin{block}{Kernel PCA}
    Linear assumption after transformation, with $\bU$ orthonormal and no  constraint on $\mathbf{F}$
    \begin{equation*}
        \Psi(\bx - \bmu) \simeq  \mathbf{F}_{1:d'} \bU_{1:d'}^\top
      \end{equation*}
   \end{block}

  \vfill

  \begin{block}{Non negative Matrix factorisation}
    Linear model assumption with $\bU$ non-negative and  $\mathbf{F}$ non-negative
    \begin{equation*}
        \bx \simeq \bmu + \mathbf{F}_{1:d'} \bU_{1:d'}^\top
      \end{equation*}
  \end{block}

  \vfill

  \paragraph{Auto-encoders} Find $\Phi$ and $\tilde\Phi$ with a neural-network!

  $\rightsquigarrow$ Fit $\bU, \mathbf{F}$ with some optimization algorithms (much more complex!)
\end{frame}


\subsection{Relation preservation point of view}

\begin{frame}
    \frametitle{Pairwise Relation}

    Focus on pairwise relation $\mathcal{R}(\bx_i, \bx_{i'})$.

    \begin{block}{Distance Preservation}
      \begin{itemize}
    \item  Construct a map $\Phi$ from the space $\Rset^{d}$ into a space $\Rset^{d'}$ of \alert{smaller dimension}:
      \begin{align*}
      \Phi:\quad & \Rset^d \to \Rset^{d'}, d' \ll d\\
               & \bx \mapsto \Phi(\bx)
      \end{align*}
      \begin{equation*}
      \text{such that} \quad \mathcal{R}(\bx_i, \bx_{i'}) \sim\mathcal{R'}(\bx'_i, \bx'_{i'})
      \end{equation*}
    \end{itemize}
  \end{block}

  \begin{block}{Multidimensional scaling}
    Try to preserve inner product related to the distance (e.g. Euclidean)
  \end{block}

  \vfill

  \begin{block}{t-SNE -- Stochastic Neighborhood Embedding}
    Try to preserve relations with close neighbors with Gaussian kernel
  \end{block}

\end{frame}


